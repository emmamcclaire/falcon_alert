{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "designing-unknown",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "import PIL.Image as Image\n",
    "import os\n",
    "\n",
    "import tensorflow.compat.v2 as tf\n",
    "import tensorflow_hub as hub\n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import itertools\n",
    "\n",
    "import pathlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "framed-rover",
   "metadata": {},
   "outputs": [],
   "source": [
    "mv2 = tf.keras.Sequential([\n",
    "    hub.KerasLayer(\"https://tfhub.dev/google/tf2-preview/mobilenet_v2/classification/4\", \n",
    "                   output_shape=[1001])\n",
    "])\n",
    "\n",
    "mv2_labels = pd.read_csv('https://storage.googleapis.com/download.tensorflow.org/data/ImageNetLabels.txt')\n",
    "mv2_labels = mv2_labels['background']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "prescribed-feelings",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mobilenet_pred(image_path, classifier, labels):\n",
    "    img = Image.open(image_path)\n",
    "    img = np.array(img)/255.0\n",
    "    result = classifier.predict(img[np.newaxis, ...])\n",
    "    predicted_label_indices = np.argpartition(result[0], -4)[-4:]\n",
    "    return labels[predicted_label_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "rising-cookie",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "44     alligator lizard\n",
       "851          television\n",
       "411               apron\n",
       "364    three-toed sloth\n",
       "Name: background, dtype: object"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# testing MobileNetV2 predictions\n",
    "\n",
    "get_mobilenet_pred('per_imgs/dataset_3/cropped/train/pos/5_06-00047-c.png', mv2, mv2_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "helpful-handbook",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "load input data\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "differential-discount",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dir = pathlib.Path('per_imgs/dataset_3/cropped/train')\n",
    "val_dir = pathlib.Path('per_imgs/dataset_3/cropped/val')\n",
    "test_dir = pathlib.Path('per_imgs/dataset_3/cropped/test')\n",
    "test_3_20_dir = pathlib.Path('per_imgs/dataset_3/resized/test_3_20')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "obvious-container",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images_dict = {\n",
    "    'pos': list(train_dir.glob('pos/*')),\n",
    "    'neg': list(train_dir.glob('neg/*')),\n",
    "}\n",
    "\n",
    "val_images_dict = {\n",
    "    'pos': list(val_dir.glob('pos/*')),\n",
    "    'neg': list(val_dir.glob('neg/*')),\n",
    "}\n",
    "\n",
    "test_images_dict = {\n",
    "    'pos': list(test_dir.glob('pos/*')),\n",
    "    'neg': list(test_dir.glob('neg/*')),\n",
    "}\n",
    "\n",
    "labels_dict = {\n",
    "    'pos': [0, 1],\n",
    "    'neg': [1, 0]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "settled-ivory",
   "metadata": {},
   "outputs": [],
   "source": [
    "def assign_x_y(images_dict):\n",
    "    \n",
    "    X, y, file_names = [], [], []\n",
    "\n",
    "    for label, images in images_dict.items():\n",
    "        for image in images:\n",
    "            try:\n",
    "                img = cv2.imread(str(image))\n",
    "                resized_img = cv2.resize(img, (224, 224))\n",
    "\n",
    "                prep_img = tf.keras.applications.mobilenet_v2.preprocess_input(img)\n",
    "\n",
    "                X.append(prep_img)\n",
    "                y.append(labels_dict[label])\n",
    "                file_names.append(image)\n",
    "            except:\n",
    "                print(image)\n",
    "            \n",
    "            # convert to np array and scale\n",
    "            \n",
    "    X = np.array(X)\n",
    "    y = np.array(y)\n",
    "    \n",
    "    return X, y, file_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "broadband-diana",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "per_imgs/dataset_3/cropped/test/pos/.DS_Store\n",
      "per_imgs/dataset_3/cropped/test/neg/.DS_Store\n"
     ]
    }
   ],
   "source": [
    "X_train, y_train, train_file_names = assign_x_y(train_images_dict)\n",
    "X_val, y_val, val_file_names = assign_x_y(val_images_dict)\n",
    "X_test, y_test, test_file_names = assign_x_y(test_images_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "laden-cooperation",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "building the model\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "anticipated-tulsa",
   "metadata": {},
   "source": [
    "1_3 had added dense layer with no preprocessing except for /255."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "least-swing",
   "metadata": {},
   "outputs": [],
   "source": [
    "t_model_1_6 = tf.keras.Sequential([\n",
    "    hub.KerasLayer('https://tfhub.dev/google/tf2-preview/mobilenet_v2/feature_vector/4', input_shape = (224, 224, 3), trainable = False),\n",
    "    layers.Dense(1000, activation='relu'),\n",
    "    layers.Dropout(0.5),\n",
    "    layers.Dense(100, activation='relu'),\n",
    "    tf.keras.layers.Dense(units = 2, activation = 'softmax')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "textile-bowling",
   "metadata": {},
   "outputs": [],
   "source": [
    "t_model_1_6.compile(optimizer=Adam(learning_rate=0.0001), loss = 'categorical_crossentropy', metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "liable-baptist",
   "metadata": {},
   "outputs": [],
   "source": [
    "callback = keras.callbacks.EarlyStopping(monitor='accuracy', min_delta = 0.01, patience=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "transsexual-accordance",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "57/57 [==============================] - 51s 831ms/step - loss: 0.5546 - accuracy: 0.7531\n",
      "Epoch 2/20\n",
      "57/57 [==============================] - 57s 998ms/step - loss: 0.1707 - accuracy: 0.9332\n",
      "Epoch 3/20\n",
      "57/57 [==============================] - 54s 937ms/step - loss: 0.1243 - accuracy: 0.9529\n",
      "Epoch 4/20\n",
      "57/57 [==============================] - 54s 952ms/step - loss: 0.1192 - accuracy: 0.9573\n",
      "Epoch 5/20\n",
      "57/57 [==============================] - 52s 911ms/step - loss: 0.0873 - accuracy: 0.9616\n",
      "Epoch 6/20\n",
      "57/57 [==============================] - 48s 841ms/step - loss: 0.0953 - accuracy: 0.9682\n",
      "Epoch 7/20\n",
      "57/57 [==============================] - 47s 825ms/step - loss: 0.0684 - accuracy: 0.9781\n",
      "Epoch 8/20\n",
      "57/57 [==============================] - 47s 824ms/step - loss: 0.0664 - accuracy: 0.9725\n",
      "Epoch 9/20\n",
      "57/57 [==============================] - 46s 804ms/step - loss: 0.0589 - accuracy: 0.9752\n",
      "Epoch 10/20\n",
      "57/57 [==============================] - 45s 792ms/step - loss: 0.0533 - accuracy: 0.9825\n",
      "Epoch 11/20\n",
      "57/57 [==============================] - 45s 786ms/step - loss: 0.0458 - accuracy: 0.9846\n",
      "Epoch 12/20\n",
      "57/57 [==============================] - 45s 780ms/step - loss: 0.0540 - accuracy: 0.9814\n",
      "Epoch 13/20\n",
      "57/57 [==============================] - 45s 782ms/step - loss: 0.0449 - accuracy: 0.9840\n",
      "Epoch 14/20\n",
      "57/57 [==============================] - 46s 800ms/step - loss: 0.0282 - accuracy: 0.9891\n",
      "Epoch 15/20\n",
      "57/57 [==============================] - 44s 776ms/step - loss: 0.0418 - accuracy: 0.9809\n",
      "Epoch 16/20\n",
      "25/57 [============>.................] - ETA: 25s - loss: 0.0577 - accuracy: 0.9857"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-159-808805561d94>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mt_model_1_6\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/opt/anaconda3/envs/ids/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1155\u001b[0m                 _r=1):\n\u001b[1;32m   1156\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1157\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1158\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1159\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/ids/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    865\u001b[0m     \u001b[0mtracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    866\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtm\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 867\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    868\u001b[0m       \u001b[0mcompiler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"xla\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"nonXla\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    869\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/ids/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    893\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 895\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    896\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/ids/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   3016\u001b[0m       (graph_function,\n\u001b[1;32m   3017\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[0;32m-> 3018\u001b[0;31m     return graph_function._call_flat(\n\u001b[0m\u001b[1;32m   3019\u001b[0m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[1;32m   3020\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/ids/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1958\u001b[0m         and executing_eagerly):\n\u001b[1;32m   1959\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1960\u001b[0;31m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[1;32m   1961\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[1;32m   1962\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
      "\u001b[0;32m~/opt/anaconda3/envs/ids/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    589\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    590\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 591\u001b[0;31m           outputs = execute.execute(\n\u001b[0m\u001b[1;32m    592\u001b[0m               \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    593\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/ids/lib/python3.8/site-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     57\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     60\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "t_model_1_6.fit(X_train, y_train, epochs = 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "incorrect-reunion",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15/15 [==============================] - 13s 753ms/step - loss: 0.5054 - accuracy: 0.8877\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.5053700804710388, 0.8876889944076538]"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t_model_1_6.evaluate(X_val, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "working-northeast",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/16 [==============================] - 12s 730ms/step - loss: 0.0950 - accuracy: 0.9469\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.09500494599342346, 0.9468504190444946]"
      ]
     },
     "execution_count": 201,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t_model_1_6.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "id": "opening-culture",
   "metadata": {},
   "outputs": [],
   "source": [
    "def df_preds(X, y, file_names):    \n",
    "    \n",
    "    preds = np.argmax(t_model_1_6.predict(X), axis = 1)\n",
    "    \n",
    "    preds_df = pd.DataFrame()\n",
    "\n",
    "    preds_df['file'] = [str(x) for x in file_names]\n",
    "    preds_df['file'] = preds_df['file'].apply(lambda x: x.split('/cropped/')[1])\n",
    "    preds_df['label'] = np.argmax(y, axis = 1)\n",
    "    preds_df['prediction'] = preds\n",
    "    preds_df['correct_pred'] = np.where(preds_df['label'] == preds_df['prediction'], True, False)\n",
    "#     bad_preds_df = preds_df[preds_df.correct_pred == False]\n",
    "    return preds_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "id": "chubby-engineering",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = df_preds(X_test, y_test, test_file_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "id": "comic-allocation",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df_pos = test_df[test_df.label == 1]\n",
    "test_df_neg = test_df[test_df.label == 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "id": "suited-mailman",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True     148\n",
      "False     24\n",
      "Name: correct_pred, dtype: int64\n",
      "True     333\n",
      "False      3\n",
      "Name: correct_pred, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(test_df_pos.correct_pred.value_counts())\n",
    "print(test_df_neg.correct_pred.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "whole-volleyball",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "model_1_4 = load_model('models/t_model_1_4.h5', custom_objects={'KerasLayer':hub.KerasLayer})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "asian-discipline",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15/15 [==============================] - 14s 837ms/step - loss: 0.4481 - accuracy: 0.8575\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.44809821248054504, 0.8574513792991638]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_1_4.evaluate(X_val, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "electrical-morris",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15/15 [==============================] - 15s 999ms/step - loss: 0.7486 - accuracy: 0.8726\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.748550534248352, 0.8725702166557312]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t_model_1_6.evaluate(X_val, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "current-contrast",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "61/61 [==============================] - 79s 1s/step - loss: 0.2219 - accuracy: 0.8783\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.22194412350654602, 0.8783013820648193]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_1_4.evaluate(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "requested-infrastructure",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "61/61 [==============================] - 85s 1s/step - loss: 0.1063 - accuracy: 0.9311\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.10627804696559906, 0.9311237931251526]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t_model_1_6.evaluate(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "id": "minus-kitty",
   "metadata": {},
   "outputs": [],
   "source": [
    "t_model_1_6.save('models/model_1_6.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "broadband-theology",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = np.argmax(t_model_1_6.predict(X_val), axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "floppy-mailing",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "try a similar feature vector approach with a bird classifier\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "atmospheric-illustration",
   "metadata": {},
   "outputs": [],
   "source": [
    "bird_classifier = tf.keras.Sequential([\n",
    "    hub.KerasLayer('https://tfhub.dev/google/experts/bit/r50x1/in21k/bird/1', input_shape = image_shape+(3,), trainable = False),\n",
    "        tf.keras.layers.Dense(units = 2, activation = 'softmax')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "delayed-peace",
   "metadata": {},
   "outputs": [],
   "source": [
    "bird_classifier.compile(optimizer=Adam(learning_rate=0.0001), loss = 'categorical_crossentropy', metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "ruled-helping",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "52/52 [==============================] - 409s 7s/step - loss: 0.9307 - accuracy: 0.5325\n",
      "Epoch 2/10\n",
      "52/52 [==============================] - 359s 7s/step - loss: 0.3238 - accuracy: 0.8642\n",
      "Epoch 3/10\n",
      "52/52 [==============================] - 331s 6s/step - loss: 0.2425 - accuracy: 0.8945\n",
      "Epoch 4/10\n",
      "52/52 [==============================] - 350s 7s/step - loss: 0.2011 - accuracy: 0.9092\n",
      "Epoch 5/10\n",
      "52/52 [==============================] - 380s 7s/step - loss: 0.2046 - accuracy: 0.9032\n",
      "Epoch 6/10\n",
      "52/52 [==============================] - 366s 7s/step - loss: 0.2122 - accuracy: 0.8984\n",
      "Epoch 7/10\n",
      "52/52 [==============================] - 314s 6s/step - loss: 0.2009 - accuracy: 0.9024\n",
      "Epoch 8/10\n",
      "52/52 [==============================] - 296s 6s/step - loss: 0.1781 - accuracy: 0.9111\n",
      "Epoch 9/10\n",
      "52/52 [==============================] - 326s 6s/step - loss: 0.1764 - accuracy: 0.9046\n",
      "Epoch 10/10\n",
      "52/52 [==============================] - 321s 6s/step - loss: 0.1930 - accuracy: 0.8995\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f8674ebd130>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bird_classifier.fit(X_train, y_train, epochs = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "authentic-qatar",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_transfer_pred(image_path, classifier = classifier):\n",
    "    img = Image.open(image_path)\n",
    "    img = np.array(img)/255.0\n",
    "    result = classifier.predict(img[np.newaxis, ...])\n",
    "    return result\n",
    "\n",
    "def make_plot_predictions(test_batch, model):\n",
    "    cm_plot_labels = ['neg', 'pos']\n",
    "    predictions = model.predict(x = test_batch, verbose = 0)\n",
    "    cm = confusion_matrix(y_true=test_batch.classes, y_pred = np.argmax(predictions, axis = -1))\n",
    "    return plot_confusion_matrix(cm = cm, classes = cm_plot_labels, title = 'Confusion Matrix')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "fabulous-designation",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15/15 [==============================] - 86s 6s/step - loss: 0.3216 - accuracy: 0.8812\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.3216049373149872, 0.8812094926834106]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bird_classifier.evaluate(X_val, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "appreciated-income",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7/7 [==============================] - 32s 4s/step - loss: 0.9965 - accuracy: 0.6080\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.9965476989746094, 0.6080402135848999]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bird_classifier.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "wireless-healthcare",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
